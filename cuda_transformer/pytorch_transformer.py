import torch
import time

# âœ… è¨­å®šè¨­å‚™
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# âœ… 24 å±¤çš„ Transformerï¼Œnhead=8
class DeepTransformer(torch.nn.Module):
    def __init__(self, d_model, nhead, num_layers):
        super(DeepTransformer, self).__init__()
        self.transformer = torch.nn.Transformer(
            d_model=d_model, nhead=nhead, num_encoder_layers=num_layers
        )

    def forward(self, src, tgt):
        return self.transformer(src, tgt)

# âœ… è¨­å®š Transformer åƒæ•¸
d_model = 512
nhead = 8  # 8 å€‹æ³¨æ„åŠ›é ­
num_layers = 24  # æ·±åº¦ 24 å±¤
learning_rate = 0.001  # è¨­å®šå­¸ç¿’ç‡
epochs = 10  # è¨“ç·´ 5 å€‹ Epoch

# âœ… åˆå§‹åŒ–æ¨¡å‹
model = DeepTransformer(d_model, nhead, num_layers).to(device)

# âœ… æå¤±å‡½æ•¸ & å„ªåŒ–å™¨
criterion = torch.nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# âœ… å‰µå»ºè¨“ç·´æ•¸æ“š
seq_len = 128
batch_size = 64
start_time = time.time()
# ğŸš€ è¨“ç·´ 5 å€‹ Epoch
for epoch in range(epochs):
    src = torch.randn(seq_len, batch_size, d_model, device=device)
    tgt = torch.randn(seq_len, batch_size, d_model, device=device)
    target = torch.randn(seq_len, batch_size, d_model, device=device)  # Ground Truth
    
    # âœ… Forward å‚³æ’­
   
    output = model(src, tgt)
    
    # âœ… è¨ˆç®— Loss
    loss = criterion(output, target)
    
    # âœ… åå‘å‚³æ’­èˆ‡æ›´æ–°æ¬Šé‡
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    


    # âœ… é¡¯ç¤ºç•¶å‰ Epoch çš„è¨“ç·´çµæœ
    print(f"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.6f}")
end_time = time.time()
print(f"æ™‚é–“: {end_time - start_time:.6f} ç§’")

print("ğŸš€ è¨“ç·´å®Œæˆï¼")
