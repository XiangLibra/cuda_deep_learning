import torch
import time
from torch.utils.cpp_extension import load

# ğŸ”¥ è¼‰å…¥ CUDA æ ¸å¿ƒ
cuda_attention = load(name="cuda_attention_deep", sources=["cuda_attention_deep.cu"], verbose=True)

# âœ… è¨­å®šè¨­å‚™
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# âœ… Transformer Attention åƒæ•¸
batch_size = 64
seq_len = 128
d_model = 512
nhead = 8
num_layers = 24  
learning_rate = 0.01

# âœ… å‰µå»ºæ•¸æ“š
Q = torch.randn(batch_size, seq_len, d_model, device=device)
K = torch.randn(batch_size, seq_len, d_model, device=device)
V = torch.randn(batch_size, seq_len, d_model, device=device)

# âœ… ç›®æ¨™è¼¸å‡º (æ¨¡æ“¬ ground truth)
target = torch.randn(batch_size, nhead, seq_len, seq_len, device=device)

# âœ… åˆå§‹åŒ–æ¬Šé‡
weights = torch.randn(d_model, d_model, device=device, requires_grad=True)

# ğŸš€ è¨“ç·´ 5 Epoch
start_time = time.time()
output = cuda_attention.attention_cuda_train(Q, K, V, target, weights, num_layers, nhead, learning_rate)
end_time = time.time()

print(f"ğŸš€ è¨“ç·´å®Œæˆï¼ç¸½æ™‚é–“: {end_time - start_time:.6f} ç§’")
