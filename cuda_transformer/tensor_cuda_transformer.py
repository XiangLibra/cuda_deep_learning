import torch
import time
from torch.utils.cpp_extension import load

# ğŸ”¥ è¼‰å…¥ CUDA Tensor Core æ ¸å¿ƒ
cuda_attention_tc = load(
    name="cuda_attention_tensorcore",
    sources=["cuda_attention_tensorcore.cu"],
    verbose=True
)

# âœ… è¨­å®šè¨­å‚™
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# âœ… Transformer åƒæ•¸è¨­å®š
batch_size = 64
seq_len = 128
d_model = 512  # ç¢ºä¿æ˜¯ 16 çš„å€æ•¸ï¼Œå¦‚ 256, 512, 1024
nhead = 8
num_layers = 24

# âœ… å‰µå»ºæ¸¬è©¦æ•¸æ“š
Q = torch.randn(batch_size, seq_len, d_model, device=device).to(torch.half).contiguous()
K = torch.randn(batch_size, seq_len, d_model, device=device).to(torch.half).contiguous()
V = torch.randn(batch_size, seq_len, d_model, device=device).to(torch.half).contiguous()

# ğŸš€ é–‹å§‹è¨ˆæ™‚ä¸¦é‹è¡Œ CUDA Tensor Core Attention
start_time = time.time()
output = cuda_attention_tc.attention_tensor_core(Q, K, V, num_layers, nhead)
end_time = time.time()

print(f"ğŸš€ ä½¿ç”¨ Tensor Cores CUDA å¤šé ­ Attention (24 å±¤, 8 Heads) é‹è¡Œæ™‚é–“: {end_time - start_time:.6f} ç§’")
